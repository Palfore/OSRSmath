{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "932f223c",
   "metadata": {},
   "source": [
    "# Solving A Simple Version of the Gauntlet\n",
    "\n",
    "This notebook describes a mathematical and programatic solution to the gauntlet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27660e",
   "metadata": {},
   "source": [
    "# Mathematical Description of Action Optimizer\n",
    "\n",
    "The `ActionOptimizer` algorithm is designed to find an optimal sequence of actions in a discrete-time, state-based scenario, commonly found in game-like environments. The approach combines recursive search with memoization and iterative deepening.\n",
    "\n",
    "## Problem Setup\n",
    "\n",
    "- Let $X(n)$ represent the state vector at discrete time step $n$.\n",
    "- $N$ is the terminal time step, which is not fixed but determined dynamically based on the state $X$.\n",
    "- At each time step, the system can perform one of $M$ actions, transforming $X(n)$ to $X(n+1)$.\n",
    "- The objective function, $O(X(N))$, calculates the score of a state at terminal time $N$, and the goal is to maximize this score.\n",
    "\n",
    "## Recursive Algorithm with Memoization\n",
    "\n",
    "### State Transformation\n",
    "- State transformation is governed by the function $evolve(X, action)$: \n",
    "  - $X_{new} = evolve(X, action)$\n",
    "- This function is part of the `Game` class, encapsulating the rules of state evolution.\n",
    "\n",
    "### Terminal State Check\n",
    "- The terminal state check is performed by $is\\_sequence\\_ended(X)$:\n",
    "  - Returns `True` if $X$ is a terminal state; otherwise, `False`.\n",
    "\n",
    "### Scoring Function\n",
    "- The scoring function $O(X)$ computes the score for a state $X$:\n",
    "  - $score = O(X)$\n",
    "- This function is also encapsulated within the `Game` class.\n",
    "\n",
    "### Recursive Function\n",
    "- The core of the algorithm is the recursive function $explore(X, current\\_sequence)$:\n",
    "  - It explores all possible action sequences starting from a given state $X$.\n",
    "  - Utilizes memoization to store and retrieve results of previously explored states.\n",
    "  - Terminates when $is\\_sequence\\_ended(X)$ is `True` and evaluates the score.\n",
    "\n",
    "### Memoization\n",
    "- A memoization dictionary, $memo$, caches the scores of explored states to reduce computational redundancy.\n",
    "- Before exploring a state, the algorithm checks $memo$ to avoid re-exploration.\n",
    "\n",
    "## Iterative Deepening Technique\n",
    "\n",
    "- Iterative deepening is implemented via $explore\\_iterative(max\\_depth)$:\n",
    "  - The algorithm incrementally deepens the exploration depth from 1 to $max\\_depth$.\n",
    "  - This ensures that shallower optimal solutions are found first, managing the depth and complexity of recursion.\n",
    "\n",
    "## Integration with the Game Class\n",
    "\n",
    "- The `Game` class abstracts the specifics of state evolution, action possibilities, terminal condition, and scoring.\n",
    "- Placeholders in the `Game` class:\n",
    "  - $evolve(X, action)$: Defines how the state $X$ changes with an action.\n",
    "  - $get\\_actions(X)$: Lists possible actions for state $X$.\n",
    "  - $is\\_sequence\\_ended(X)$: Determines if $X$ is a terminal state.\n",
    "  - $compute\\_score(X)$: Calculates the score of state $X$.\n",
    "\n",
    "## Execution\n",
    "\n",
    "- Initialize `Game` with the initial state $X(0)$.\n",
    "- Instantiate `ActionOptimizer` with the `Game` object.\n",
    "- Invoke `find_optimal_sequences` to compute the optimal sequence of actions, choosing between recursive and iterative deepening modes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13518d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Any\n",
    "\n",
    "GameState = Tuple[Any, ...]  # Type alias for the game state\n",
    "ActionOutcome = Tuple[GameState, float]  # Type alias for action probability outcomes\n",
    "ActionSequence = List[int]  # Type alias for a sequence of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e0925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionOptimizer:\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "        self.optimal_sequences = []\n",
    "        self.max_score = float('-inf')\n",
    "        self.memo = {}\n",
    "\n",
    "    def explore_recursive(self):\n",
    "        def explore(X, current_sequence):\n",
    "            X_key = tuple(X)\n",
    "            if X_key in self.memo:\n",
    "                return self.memo[X_key]\n",
    "\n",
    "            if self.game.is_sequence_ended(X):\n",
    "                score = self.game.compute_score(X)\n",
    "                self.update_optimal_sequences(score, current_sequence)\n",
    "                self.memo[X_key] = score\n",
    "                return score\n",
    "\n",
    "            for action in self.game.get_actions(X):\n",
    "                new_X = self.game.evolve(X, action)\n",
    "                current_sequence.append(action)\n",
    "                explore(new_X, current_sequence)\n",
    "                current_sequence.pop()\n",
    "\n",
    "            self.memo[X_key] = None\n",
    "\n",
    "        explore(self.game.initial_state, [])\n",
    "\n",
    "    def explore_iterative(self, max_depth):\n",
    "        def explore_depth_limited(X, current_sequence, depth):\n",
    "            if depth == 0:\n",
    "                return\n",
    "\n",
    "            X_key = tuple(X)\n",
    "            if X_key in self.memo:\n",
    "                return self.memo[X_key]\n",
    "\n",
    "            if self.game.is_sequence_ended(X):\n",
    "                score = self.game.compute_score(X)\n",
    "                self.update_optimal_sequences(score, current_sequence)\n",
    "                self.memo[X_key] = score\n",
    "                return score\n",
    "\n",
    "            for action in self.game.get_actions(X):\n",
    "                new_X = self.game.evolve(X, action)\n",
    "                current_sequence.append(action)\n",
    "                explore_depth_limited(new_X, current_sequence, depth - 1)\n",
    "                current_sequence.pop()\n",
    "\n",
    "            self.memo[X_key] = None\n",
    "\n",
    "        for depth in range(1, max_depth + 1):\n",
    "            explore_depth_limited(self.game.initial_state, [], depth)\n",
    "\n",
    "    def update_optimal_sequences(self, score, current_sequence):\n",
    "        if score > self.max_score:\n",
    "            self.max_score = score\n",
    "            self.optimal_sequences = [current_sequence.copy()]\n",
    "        elif score == self.max_score:\n",
    "            self.optimal_sequences.append(current_sequence.copy())\n",
    "\n",
    "    def find_optimal_sequences(self, iterative_deepening=False, max_depth=100):\n",
    "        if iterative_deepening:\n",
    "            self.explore_iterative(max_depth)\n",
    "        else:\n",
    "            self.explore_recursive()\n",
    "        return self.optimal_sequences, self.max_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eab4805",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Game' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_384/1961119916.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mActionSequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Type alias for a sequence of actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mProbabilisticActionOptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_384/1961119916.py\u001b[0m in \u001b[0;36mProbabilisticActionOptimizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mProbabilisticActionOptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimal_sequences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mActionSequence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Game' is not defined"
     ]
    }
   ],
   "source": [
    "ActionSequence = List[int]  # Type alias for a sequence of actions\n",
    "\n",
    "class ProbabilisticActionOptimizer:\n",
    "    def __init__(self, game: Game):\n",
    "        self.game = game\n",
    "        self.optimal_sequences: List[ActionSequence] = []\n",
    "        self.max_expected_score: float = float('-inf')\n",
    "\n",
    "    def expected_score(self, sequence: ActionSequence, probability: float) -> float:\n",
    "        assert self.game.is_sequence_ended(sequence[-1]), \\\n",
    "            \"Expected score should be calculated at the end of the game sequence.\"\n",
    "        \n",
    "        final_state_score = self.game.compute_score(sequence[-1])\n",
    "        weighted_score = final_state_score * probability\n",
    "        return weighted_score\n",
    "\n",
    "    def explore(self, X: GameState, current_sequence: ActionSequence, probability: float = 1.0):\n",
    "        if self.game.is_sequence_ended(X):\n",
    "            exp_score = self.expected_score(current_sequence, probability)\n",
    "            if exp_score > self.max_expected_score:\n",
    "                self.max_expected_score = exp_score\n",
    "                self.optimal_sequences = [current_sequence.copy()]\n",
    "            elif exp_score == self.max_expected_score:\n",
    "                self.optimal_sequences.append(current_sequence.copy())\n",
    "            return\n",
    "\n",
    "        for action in self.game.get_actions(X):\n",
    "            for new_X, action_probability in self.game.probabilistic_evolve(X, action):\n",
    "                current_sequence.append(action)\n",
    "                new_probability = probability * action_probability\n",
    "                self.explore(new_X, current_sequence, new_probability)\n",
    "                current_sequence.pop()\n",
    "\n",
    "    def find_optimal_sequences(self) -> Tuple[List[ActionSequence], float]:\n",
    "        self.explore(self.game.initial_X, [])\n",
    "        return self.optimal_sequences, self.max_expected_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e5848",
   "metadata": {},
   "source": [
    "# Specific Game Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2783387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameState:\n",
    "    def __init__(self, initial_state: Any):\n",
    "        self.state = initial_state\n",
    "\n",
    "    def move(self, action: int) -> 'GameState':\n",
    "        # Implement the logic to modify the game state based on the action\n",
    "        # Return a new GameState object representing the new state\n",
    "        # Example implementation (modify as needed):\n",
    "        new_state = ... # Logic to determine the new state\n",
    "        return GameState(new_state)\n",
    "\n",
    "    # Add additional methods as needed for state management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3ae3863",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self, initial_state: Any):\n",
    "        self.initial_X = GameState(initial_state)\n",
    "\n",
    "    def get_actions(self, X: GameState) -> List[int]:\n",
    "        # Return possible actions based on state X\n",
    "        return [0, 1, 2]  # Example actions\n",
    "\n",
    "    def probabilistic_evolve(self, X: GameState, action: int) -> List[Tuple[GameState, float]]:\n",
    "        # Given a state and an action, return a list of tuples (new_X, probability)\n",
    "        # for each possible outcome\n",
    "        return [(X.move(action), 0.5), (X.move(action), 0.5)]  # Example outcomes\n",
    "\n",
    "    def is_sequence_ended(self, X: GameState) -> bool:\n",
    "        # Determine if the current state signifies the end of a sequence\n",
    "        return False  # Example condition\n",
    "\n",
    "    def compute_score(self, state: GameState) -> float:\n",
    "        # Calculate and return the score for the given game state\n",
    "        return 0.0  # Example score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f5e548e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initial_state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_384/841244938.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example Usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Initialize the Game with the initial state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActionOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Using Recursive Approach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'initial_state' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example Usage\n",
    "game = Game(initial_state)  # Initialize the Game with the initial state\n",
    "optimizer = ActionOptimizer(game)\n",
    "\n",
    "# Using Recursive Approach\n",
    "optimal_sequences, optimal_score = optimizer.find_optimal_sequences()\n",
    "print(\"Recursive Approach - Optimal sequences:\", optimal_sequences)\n",
    "print(\"Optimal score:\", optimal_score)\n",
    "\n",
    "# Using Iterative Deepening Approach\n",
    "optimal_sequences, optimal_score = optimizer.find_optimal_sequences(iterative_deepening=True, max_depth=100)\n",
    "print(\"Iterative Deepening - Optimal sequences:\", optimal_sequences)\n",
    "print(\"Optimal score:\", optimal_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a998d99",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ProbabilisticActionOptimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_384/4088468842.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProbabilisticActionOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moptimal_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_optimal_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ProbabilisticActionOptimizer' is not defined"
     ]
    }
   ],
   "source": [
    "initial_state = ...  # Define the initial state appropriate for your game\n",
    "\n",
    "game = Game(initial_state)\n",
    "optimizer = ProbabilisticActionOptimizer(game)\n",
    "\n",
    "optimal_sequences, optimal_score = optimizer.find_optimal_sequences()\n",
    "\n",
    "print(\"Optimal sequences:\", optimal_sequences)\n",
    "print(\"Optimal expected score:\", optimal_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
